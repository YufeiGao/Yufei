{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import multiprocessing\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "LARGE_FILE = \"C:\\Users\\Administrator\\Desktop\\practicum_regression\\loan_data_no_current_converted.csv\"\n",
    "CHUNKSIZE = 100000 # processing 100,000 rows at a time\n",
    "reader = pd.read_csv(LARGE_FILE, chunksize=CHUNKSIZE, low_memory=False)\n",
    "frames = []\n",
    "for df in reader:\n",
    "    frames.append(df)\n",
    "loan_data = pd.concat(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "# n is the number of resampled set\n",
    "# this is a function with replacement\n",
    "# rand_state is an int, you can change it in a for-loop to generate\n",
    "# more random dataset\n",
    "def resample_df(X, Y, n, rand_state):\n",
    "    X_sampled, Y_sampled = resample(X, Y, random_state=rand_state, n_samples=n, replace=False)\n",
    "    return X_sampled, Y_sampled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manually generate validation dataset (in case of clf is not useful...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split , cross_val_score\n",
    "def split(x,y,rand=0):\n",
    "        \n",
    "        y = np.ravel(y)\n",
    "        x_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.25,random_state=rand)\n",
    "        \n",
    "        return x_train, x_test, y_train, y_test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_choice(df, y, n):\n",
    "    if (len(df) % n != 0):\n",
    "        df1 = df[0:len(df)-len(df)%n]\n",
    "        y1 = y[0:len(y)-len(y)%n]\n",
    "    return df1, y1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_dataset(df, y, n, m):\n",
    "    y=pd.Series(y)\n",
    "    # by calling the function split, y will become array type\n",
    "    interval_len=len(df)/n\n",
    "    if m == 1:\n",
    "        x_test = df[0:interval_len]\n",
    "        y_test = y[0:interval_len]\n",
    "        x_train = df[interval_len:len(df)]\n",
    "        y_train = y[interval_len:len(df)]\n",
    "    elif m == n:\n",
    "        x_test = df[(n-1)*interval_len:len(df)]\n",
    "        y_test = y[(n-1)*interval_len:len(df)]\n",
    "        x_train = df[0:(n-1)*interval_len]\n",
    "        y_train = y[0:(n-1)*interval_len]\n",
    "    else:\n",
    "        x_test = df[(m-1)*interval_len:m*interval_len]\n",
    "        x_train1 = df[0:(m-1)*interval_len]\n",
    "        x_train2 = df[m*interval_len:len(df)]\n",
    "        frames1 = [x_train1, x_train2]\n",
    "        x_train = pd.concat(frames1)\n",
    "        \n",
    "        y_test = y[(m-1)*interval_len:m*interval_len]\n",
    "        y_train1 = y[0:(m-1)*interval_len]\n",
    "        y_train2 = y[m*interval_len:len(df)]\n",
    "        frames2 = [y_train1, y_train2]\n",
    "        y_train = pd.concat(frames2)\n",
    "    return x_train, x_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def process_zip():\n",
    "        #dealing with zipcode\n",
    "        zip_data=pd.read_csv('ZIP.csv')\n",
    "        zip_data['Zip']=zip_data['Zip'].astype(str)\n",
    "        for i in range(len(zip_data['Zip'])):\n",
    "            if len(zip_data['Zip'][i])==4:\n",
    "                zip_data['Zip'][i]='0'+zip_data['Zip'][i]\n",
    "        zip_data['Zip']=[zip_data['Zip'][i][0:3] for i in range(len(zip_data))] \n",
    "        zip_data.iloc[:,-3:]=zip_data[['Median','Mean','Pop']].apply(lambda x: x.str.replace(',',''))\n",
    "        for i in range(1,4):\n",
    "            zip_data.iloc[:,-i]=pd.to_numeric(zip_data.iloc[:,-i],errors='coerce')\n",
    "        zip_data['weight']=zip_data['Pop']/zip_data.groupby('Zip')['Pop'].transform(sum)\n",
    "        zip_data['new_mean']=zip_data['Mean']*zip_data['weight']\n",
    "        zip_data['new_median']=zip_data['Median']*zip_data['weight']\n",
    "        zip_new=pd.DataFrame()\n",
    "        zip_new=zip_data.groupby('Zip')['new_mean','new_median'].sum()\n",
    "        return zip_new\n",
    "        \n",
    "    def readcsv():\n",
    "        LARGE_FILE = \"C:\\Users\\Administrator\\Desktop\\practicum_regression\\loan_data_no_current_converted.csv\"\n",
    "        CHUNKSIZE = 100000 # processing 100,000 rows at a time\n",
    "        reader = pd.read_csv(LARGE_FILE, chunksize=CHUNKSIZE, low_memory=False)\n",
    "        frames = []\n",
    "        for df in reader:\n",
    "            frames.append(df)\n",
    "        loan_data = pd.concat(frames)\n",
    "        return loan_data   \n",
    "    def cleaning(df,zip_new,keep_desc=True,categorical_to_binary=True):\n",
    "        df=df.dropna(axis=0,how='all')\n",
    "        drop_list=['emp_title','title','earliest_cr_line','desc','issue_d','id','member_id','url','grade','sub_grade',\n",
    "                   'int_rate','avg_cur_bal','addr_state','funded_amnt','funded_amnt_inv','collection_recovery_fee',\n",
    "                   'collections_12_mths_ex_med','mths_since_last_major_derog','next_pymnt_d','recoveries','total_pymnt',\n",
    "                   'total_pymnt_inv','total_rec_int','issue_d',' last_credit_pull_d','last_pymnt_d','last_credit_pull_d',\n",
    "                  'total_rec_prncp','settlement_status','hardship_loan_status','hardship_status','debt_settlement_flag',\n",
    "                   'verification_status','total_rec_late_fee']\n",
    "        df.drop(drop_list,inplace=True,axis=1,errors='ignore')\n",
    "        #deal with percentage mark\n",
    "        df['revol_util']=df['revol_util'].replace('%','',regex=True).astype('float')/100\n",
    "        #merge zipcode with census data\n",
    "        df['zip_code']=df['zip_code'].apply(lambda x: x[:3])\n",
    "        df=df.join(zip_new,on='zip_code')\n",
    "        df.drop('zip_code',inplace=True,axis=1)\n",
    "        #drop the observation that was missing for ALL field\n",
    "        df=df.dropna(axis=0,how='all')\n",
    "        #drop the features for which greater than 10% of the loans were missing data for\n",
    "        num_rows=df.count(axis=0)\n",
    "        df=df.iloc[:,(num_rows>=0.9*len(df)).tolist()]\n",
    "        #drop the observation that was missing for any field\n",
    "        df=df.dropna(axis=0,how='any')\n",
    "        #label the dataset to create y\n",
    "        #0:fully paid, does not meet policy:fully paid\n",
    "        #1:Does not meet the credit policy. Status:Charged Off\",default,charge of\n",
    "        #delete grace and late\n",
    "        df=df[(True^df['loan_status'].isin([4]))] \n",
    "        df=df[(True^df['loan_status'].isin([5]))]\n",
    "        df=df[(True^df['loan_status'].isin([6]))] \n",
    "        #label the dataset to create y\n",
    "        y=df['loan_status'].replace(1,0)\n",
    "        # rename to let only 0 and 1 left\n",
    "        y=y.replace(2,0)\n",
    "        y=y.replace(3,1)\n",
    "        y=y.replace(7,1)\n",
    "        y=y.replace(8,1)\n",
    "        df=df.drop(['loan_status'],axis=1) \n",
    "        return df,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "zipdata = process_zip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = readcsv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = cleaning(df,zipdata,keep_desc=False,categorical_to_binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = resample(x, y, random_state=0, n_samples=len(x),replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bool_cols(df,isbool=True):\n",
    "    bool_cols=[]\n",
    "    for col in df:\n",
    "        if isbool==True:\n",
    "            if df[col].dropna().value_counts().index.isin([0,1]).all():\n",
    "                bool_cols.append(col)\n",
    "        else:\n",
    "            if not df[col].dropna().value_counts().index.isin([0,1]).all():\n",
    "                bool_cols.append(col)\n",
    "    return bool_cols\n",
    "# this above step is to facilitate normalization later\n",
    "# method two\n",
    "def not_bi(x):\n",
    "    not_bi=[]\n",
    "    for i in list(x):\n",
    "        u=x[i].unique()\n",
    "        if not (0 in u and 1 in u and len(u)==2): #if not binary\n",
    "            not_bi.append(i)\n",
    "    return not_bi\n",
    "    \n",
    "def reg(x_train, y_train):\n",
    "           \n",
    "    model = LogisticRegression(penalty='l2',class_weight='balanced',solver='sag',n_jobs=-1)\n",
    "    \n",
    "    model = model.fit(x_train, y_train)\n",
    "        \n",
    "    return model\n",
    "    \n",
    "def ModelValuation(x_test,y_test,model):\n",
    "        \n",
    "    probs = model.predict_proba(x_test)\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y_test, probs[:, 1])\n",
    "        \n",
    "    plt.figure(1)\n",
    "    plt.plot(fpr, tpr, label='LogisticRegression')\n",
    "    plt.xlabel('False positive rate')\n",
    "    plt.ylabel('True positive rate')\n",
    "    plt.title('ROC curve')\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()\n",
    "        \n",
    "    print(\"Area Under the Curve (AUC) from prediction score is %f\" % metrics.roc_auc_score(y_test, probs[:, 1]))\n",
    "    \n",
    "    return None  \n",
    "    \n",
    "def y_pred(x_test,threshold=0.5):\n",
    "        \n",
    "    if threshold == 0.5:\n",
    "        y_predicted = model.predict(x_test)\n",
    "    else:\n",
    "        probs = model.predict_proba(x_test)\n",
    "        y_predicted = np.array(probs[:,1] >= threshold).astype(int)\n",
    "        \n",
    "    return y_predicted    \n",
    "    \n",
    "def GetScores(y_test,y_predicted):\n",
    "    #G means score \n",
    "    CM = metrics.confusion_matrix(y_test, y_predicted)\n",
    "    TN = CM[0,0]\n",
    "    FN = CM[1,0]\n",
    "    TP = CM[1,1]\n",
    "    FP = CM[0,1]\n",
    "        \n",
    "    sensitivity = float(TP)/float(TP+FN)\n",
    "    specificity = float(TN)/float(TN+FP)\n",
    "    G = np.sqrt(sensitivity*specificity)\n",
    "    print(\"G score is %f\" % G)\n",
    "    print(\"Specificity is %f\" % specificity)\n",
    "        \n",
    "    # Generate and display different evaluation metrics\n",
    "    print(\"Mean accuracy score is %f\" % metrics.accuracy_score(y_test, y_predicted))\n",
    "          \n",
    "    print(\"Confusion Marix\")\n",
    "    print(CM)\n",
    "        \n",
    "    return specificity , G\n",
    "        \n",
    "# Convenience function to plot confusion matrix\n",
    "def confusion(y_test,y_predicted,title):\n",
    "        \n",
    "    # Define names for the three Iris types\n",
    "    names = ['Default', 'Not Default']\n",
    "    \n",
    "    # Make a 2D histogram from the test and result arrays\n",
    "    pts, xe, ye = np.histogram2d(y_test, y_predicted, bins=2)\n",
    "    \n",
    "    # For simplicity we create a new DataFrame\n",
    "    pd_pts = pd.DataFrame(pts.astype(int), index=names, columns=names )\n",
    "        \n",
    "    # Display heatmap and add decorations\n",
    "    hm = sns.heatmap(pd_pts, annot=True, fmt=\"d\")\n",
    "    hm.axes.set_title(title)\n",
    "        \n",
    "    return None\n",
    "            \n",
    "def find_threshold(x_test,y_test):\n",
    "    \n",
    "    probs = model.predict_proba(x_test)\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y_test, probs[:, 1])\n",
    "        \n",
    "    sensitivity = tpr\n",
    "    specificity = 1 - fpr\n",
    "    G = np.sqrt(sensitivity*specificity)\n",
    "        \n",
    "    plt.figure(2)\n",
    "    plt.plot(thresholds,G)\n",
    "    plt.xlabel('Thresholds')\n",
    "    plt.ylabel('G-Scores')\n",
    "    plt.title('G-Scores with different thresholds')\n",
    "    plt.show()\n",
    "        \n",
    "        \n",
    "    print(\"The highest G score is %f with threshold at %f\" % (np.amax(G),thresholds[np.argmax(G)]) )\n",
    "        \n",
    "    return thresholds[np.argmax(G)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_bi = not_bi(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test=split(x,y,rand=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,y_train=drop_choice(x_train, y_train, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(579060, 579060)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_train),len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(193022, 193022)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_test),len(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trail 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split\n",
    "#The only difference is the train_test split is substituted by this function every time\n",
    "# the evaluation function also works each time\n",
    "x_val_train, x_val_test, y_val_train, y_val_test = validation_dataset(x_train,y_train,5,2)\n",
    "# Normalize\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(x_train[not_bi]) \n",
    "\n",
    "x_val_train_scaled=x_val_train\n",
    "x_test_scaled=x_test\n",
    "\n",
    "x_train_scaled[not_bi] = scaler.transform(x_train[not_bi])\n",
    "x_test_scaled[not_bi]  = scaler.transform(x_test[not_bi])\n",
    "\n",
    "# Fit model\n",
    "model = reg(x_train_scaled,y_train)\n",
    "# Evaluate model\n",
    "ModelValuation(x_test_scaled,y_test,model)\n",
    "y_predicted = y_pred(x_test_scaled,threshold=0.5)\n",
    "spec , G = GetScores(y_test,y_predicted)\n",
    "confusion(y_test,y_predicted,'Default Confusion Matrix')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
